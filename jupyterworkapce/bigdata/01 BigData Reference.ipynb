{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/home/ec2-user/spark-3.1.1-bin-hadoop3.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark import SQLContext, SparkConf\n",
    "from pyspark.sql import SparkSession "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('cars.csv',header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mpg', 'cyl', 'disp', 'hp', 'wt', 'acc', 'yr', 'origin', 'car_name']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+---+----+----+---+------+--------------------+\n",
      "|mpg|cyl|disp| hp|  wt| acc| yr|origin|            car_name|\n",
      "+---+---+----+---+----+----+---+------+--------------------+\n",
      "| 18|  8| 307|130|3504|  12| 70|     1|chevrolet chevell...|\n",
      "| 15|  8| 350|165|3693|11.5| 70|     1|   buick skylark 320|\n",
      "| 18|  8| 318|150|3436|  11| 70|     1|  plymouth satellite|\n",
      "| 16|  8| 304|150|3433|  12| 70|     1|       amc rebel sst|\n",
      "| 17|  8| 302|140|3449|10.5| 70|     1|         ford torino|\n",
      "| 15|  8| 429|198|4341|  10| 70|     1|    ford galaxie 500|\n",
      "| 14|  8| 454|220|4354|   9| 70|     1|    chevrolet impala|\n",
      "| 14|  8| 440|215|4312| 8.5| 70|     1|   plymouth fury iii|\n",
      "| 14|  8| 455|225|4425|  10| 70|     1|    pontiac catalina|\n",
      "| 15|  8| 390|190|3850| 8.5| 70|     1|  amc ambassador dpl|\n",
      "| 15|  8| 383|170|3563|  10| 70|     1| dodge challenger se|\n",
      "| 14|  8| 340|160|3609|   8| 70|     1|  plymouth 'cuda 340|\n",
      "| 15|  8| 400|150|3761| 9.5| 70|     1|chevrolet monte c...|\n",
      "| 14|  8| 455|225|3086|  10| 70|     1|buick estate wago...|\n",
      "| 24|  4| 113| 95|2372|  15| 70|     3|toyota corona mar...|\n",
      "| 22|  6| 198| 95|2833|15.5| 70|     1|     plymouth duster|\n",
      "| 18|  6| 199| 97|2774|15.5| 70|     1|          amc hornet|\n",
      "| 21|  6| 200| 85|2587|  16| 70|     1|       ford maverick|\n",
      "| 27|  4|  97| 88|2130|14.5| 70|     3|        datsun pl510|\n",
      "| 26|  4|  97| 46|1835|20.5| 70|     2|volkswagen 1131 d...|\n",
      "+---+---+----+---+----+----+---+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+--------------------+\n",
      "|summary|               mpg|               cyl|              disp|                hp|               wt|               acc|                yr|            origin|            car_name|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+--------------------+\n",
      "|  count|               398|               398|               398|               398|              398|               398|               398|               398|                 398|\n",
      "|   mean|23.514572864321615| 5.454773869346734|193.42587939698493|104.46938775510205|2970.424623115578|15.568090452261291| 76.01005025125629|1.5728643216080402|                null|\n",
      "| stddev| 7.815984312565783|1.7010042445332123|104.26983817119587| 38.49115993282846|846.8417741973268| 2.757688929812676|3.6976266467325862|0.8020548777266148|                null|\n",
      "|    min|                10|                 3|               100|               100|             1613|                10|                70|                 1|amc ambassador br...|\n",
      "|    max|                 9|                 8|                98|                 ?|             5140|               9.5|                82|                 3|    vw rabbit custom|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- mpg: string (nullable = true)\n",
      " |-- cyl: string (nullable = true)\n",
      " |-- disp: string (nullable = true)\n",
      " |-- hp: string (nullable = true)\n",
      " |-- wt: string (nullable = true)\n",
      " |-- acc: string (nullable = true)\n",
      " |-- yr: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- car_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(mpg='18', cyl='8', disp='307', hp='130', wt='3504', acc='12', yr='70', origin='1', car_name='chevrolet chevelle malibu'),\n",
       " Row(mpg='15', cyl='8', disp='350', hp='165', wt='3693', acc='11.5', yr='70', origin='1', car_name='buick skylark 320'),\n",
       " Row(mpg='18', cyl='8', disp='318', hp='150', wt='3436', acc='11', yr='70', origin='1', car_name='plymouth satellite')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            car_name|\n",
      "+--------------------+\n",
      "|chevrolet chevell...|\n",
      "|   buick skylark 320|\n",
      "|  plymouth satellite|\n",
      "|       amc rebel sst|\n",
      "|         ford torino|\n",
      "|    ford galaxie 500|\n",
      "|    chevrolet impala|\n",
      "|   plymouth fury iii|\n",
      "|    pontiac catalina|\n",
      "|  amc ambassador dpl|\n",
      "| dodge challenger se|\n",
      "|  plymouth 'cuda 340|\n",
      "|chevrolet monte c...|\n",
      "|buick estate wago...|\n",
      "|toyota corona mar...|\n",
      "|     plymouth duster|\n",
      "|          amc hornet|\n",
      "|       ford maverick|\n",
      "|        datsun pl510|\n",
      "|volkswagen 1131 d...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('car_name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a SQL view and using it for query\n",
    "df.createOrReplaceTempView('cyl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_results = spark.sql(\"SELECT max(cyl) FROM cyl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|max(cyl)|\n",
      "+--------+\n",
      "|       8|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_results = spark.sql(\"SELECT * FROM cyl where cyl<6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+---+----+----+---+------+--------------------+\n",
      "|mpg|cyl|disp| hp|  wt| acc| yr|origin|            car_name|\n",
      "+---+---+----+---+----+----+---+------+--------------------+\n",
      "| 24|  4| 113| 95|2372|  15| 70|     3|toyota corona mar...|\n",
      "| 27|  4|  97| 88|2130|14.5| 70|     3|        datsun pl510|\n",
      "| 26|  4|  97| 46|1835|20.5| 70|     2|volkswagen 1131 d...|\n",
      "| 25|  4| 110| 87|2672|17.5| 70|     2|         peugeot 504|\n",
      "| 24|  4| 107| 90|2430|14.5| 70|     2|         audi 100 ls|\n",
      "| 25|  4| 104| 95|2375|17.5| 70|     2|            saab 99e|\n",
      "| 26|  4| 121|113|2234|12.5| 70|     2|            bmw 2002|\n",
      "| 27|  4|  97| 88|2130|14.5| 71|     3|        datsun pl510|\n",
      "| 28|  4| 140| 90|2264|15.5| 71|     1| chevrolet vega 2300|\n",
      "| 25|  4| 113| 95|2228|  14| 71|     3|       toyota corona|\n",
      "| 25|  4|  98|  ?|2046|  19| 71|     1|          ford pinto|\n",
      "| 22|  4| 140| 72|2408|  19| 71|     1| chevrolet vega (sw)|\n",
      "| 23|  4| 122| 86|2220|  14| 71|     1|  mercury capri 2000|\n",
      "| 28|  4| 116| 90|2123|  14| 71|     2|           opel 1900|\n",
      "| 30|  4|  79| 70|2074|19.5| 71|     2|         peugeot 304|\n",
      "| 30|  4|  88| 76|2065|14.5| 71|     2|           fiat 124b|\n",
      "| 31|  4|  71| 65|1773|  19| 71|     3| toyota corolla 1200|\n",
      "| 35|  4|  72| 69|1613|  18| 71|     3|         datsun 1200|\n",
      "| 27|  4|  97| 60|1834|  19| 71|     2|volkswagen model 111|\n",
      "| 26|  4|  91| 70|1955|20.5| 71|     1|    plymouth cricket|\n",
      "+---+---+----+---+----+----+---+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----+---+----+----+---+------+--------------------+\n",
      "| mpg|cyl|disp| hp|  wt| acc| yr|origin|            car_name|\n",
      "+----+---+----+---+----+----+---+------+--------------------+\n",
      "|  26|  4|  97| 46|1835|20.5| 70|     2|volkswagen 1131 d...|\n",
      "|  26|  4|  97| 46|1950|  21| 73|     2|volkswagen super ...|\n",
      "|  29|  4|  68| 49|1867|19.5| 73|     2|            fiat 128|\n",
      "|43.1|  4|  90| 48|1985|21.5| 78|     2|volkswagen rabbit...|\n",
      "|44.3|  4|  90| 48|2085|21.7| 80|     2|vw rabbit c (diesel)|\n",
      "|43.4|  4|  90| 48|2335|23.7| 80|     2|  vw dasher (diesel)|\n",
      "+----+---+----+---+----+----+---+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"hp<50\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| hp|            car_name|\n",
      "+---+--------------------+\n",
      "| 46|volkswagen 1131 d...|\n",
      "| 46|volkswagen super ...|\n",
      "| 49|            fiat 128|\n",
      "| 48|volkswagen rabbit...|\n",
      "| 48|vw rabbit c (diesel)|\n",
      "| 48|  vw dasher (diesel)|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"hp<50\").select('hp','car_name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----+---+----+----+---+------+--------------------+\n",
      "| mpg|cyl|disp| hp|  wt| acc| yr|origin|            car_name|\n",
      "+----+---+----+---+----+----+---+------+--------------------+\n",
      "|  26|  4|  97| 46|1950|  21| 73|     2|volkswagen super ...|\n",
      "|43.1|  4|  90| 48|1985|21.5| 78|     2|volkswagen rabbit...|\n",
      "|44.3|  4|  90| 48|2085|21.7| 80|     2|vw rabbit c (diesel)|\n",
      "|43.4|  4|  90| 48|2335|23.7| 80|     2|  vw dasher (diesel)|\n",
      "+----+---+----+---+----+----+---+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter( (df[\"hp\"] < 50) & (df['wt'] > 1900) ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.filter( (df[\"hp\"] < 50) & (df['wt'] > 1900) ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(mpg='43.1', cyl='4', disp='90', hp='48', wt='1985', acc='21.5', yr='78', origin='2', car_name='volkswagen rabbit custom diesel')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mpg': '43.1',\n",
       " 'cyl': '4',\n",
       " 'disp': '90',\n",
       " 'hp': '48',\n",
       " 'wt': '1985',\n",
       " 'acc': '21.5',\n",
       " 'yr': '78',\n",
       " 'origin': '2',\n",
       " 'car_name': 'volkswagen rabbit custom diesel'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1].asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.1\n",
      "4\n",
      "90\n",
      "48\n",
      "1985\n",
      "21.5\n",
      "78\n",
      "2\n",
      "volkswagen rabbit custom diesel\n"
     ]
    }
   ],
   "source": [
    "for item in result[1]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|cyl|count|\n",
      "+---+-----+\n",
      "|  3|    4|\n",
      "|  8|  103|\n",
      "|  5|    3|\n",
      "|  6|   84|\n",
      "|  4|  204|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"cyl\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----+---+----+----+---+------+--------------------+\n",
      "| mpg|cyl|disp| hp|  wt| acc| yr|origin|            car_name|\n",
      "+----+---+----+---+----+----+---+------+--------------------+\n",
      "|  13|  8| 360|175|3821|  11| 73|     1|amc ambassador br...|\n",
      "|  15|  8| 390|190|3850| 8.5| 70|     1|  amc ambassador dpl|\n",
      "|  17|  8| 304|150|3672|11.5| 72|     1|  amc ambassador sst|\n",
      "|19.4|  6| 232| 90|3210|17.2| 78|     1|         amc concord|\n",
      "|24.3|  4| 151| 90|3003|20.1| 80|     1|         amc concord|\n",
      "|18.1|  6| 258|120|3410|15.1| 78|     1|     amc concord d/l|\n",
      "|  23|  4| 151|  ?|3035|20.5| 82|     1|      amc concord dl|\n",
      "|20.2|  6| 232| 90|3265|18.2| 79|     1|    amc concord dl 6|\n",
      "|  20|  6| 232|100|2914|  16| 75|     1|         amc gremlin|\n",
      "|  19|  6| 232|100|2634|  13| 71|     1|         amc gremlin|\n",
      "|  21|  6| 199| 90|2648|  15| 70|     1|         amc gremlin|\n",
      "|  18|  6| 232|100|2789|  15| 73|     1|         amc gremlin|\n",
      "|  18|  6| 199| 97|2774|15.5| 70|     1|          amc hornet|\n",
      "|  18|  6| 232|100|2945|  16| 73|     1|          amc hornet|\n",
      "|  19|  6| 232|100|2901|  16| 74|     1|          amc hornet|\n",
      "|22.5|  6| 232| 90|3085|17.6| 76|     1|          amc hornet|\n",
      "|  18|  6| 258|110|2962|13.5| 71|     1|amc hornet sporta...|\n",
      "|  16|  6| 258|110|3632|  18| 74|     1|         amc matador|\n",
      "|  14|  8| 304|150|3672|11.5| 73|     1|         amc matador|\n",
      "|  15|  6| 258|110|3730|  19| 75|     1|         amc matador|\n",
      "+----+---+----+---+----+----+---+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(\"car_name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearRegression in pySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = spark.read.format(\"libsvm\").load(\"sample_linear_regression_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in the split between training/test as a list.\n",
    "# No correct, but generally 70/30 or 60/40 splits are used. \n",
    "# Depending on how much data you have and how unbalanced it is.\n",
    "train_data,test_data = all_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|              label|            features|\n",
      "+-------------------+--------------------+\n",
      "|-28.046018037776633|(10,[0,1,2,3,4,5,...|\n",
      "|-23.487440120936512|(10,[0,1,2,3,4,5,...|\n",
      "|-22.949825936196074|(10,[0,1,2,3,4,5,...|\n",
      "|-22.837460416919342|(10,[0,1,2,3,4,5,...|\n",
      "|-21.432387764165806|(10,[0,1,2,3,4,5,...|\n",
      "|-20.057482615789212|(10,[0,1,2,3,4,5,...|\n",
      "|-19.884560774273424|(10,[0,1,2,3,4,5,...|\n",
      "|-19.872991038068406|(10,[0,1,2,3,4,5,...|\n",
      "|-19.782762789614537|(10,[0,1,2,3,4,5,...|\n",
      "| -19.66731861537172|(10,[0,1,2,3,4,5,...|\n",
      "|-19.402336030214553|(10,[0,1,2,3,4,5,...|\n",
      "|-17.803626188664516|(10,[0,1,2,3,4,5,...|\n",
      "|-17.428674570939506|(10,[0,1,2,3,4,5,...|\n",
      "| -17.32672073267595|(10,[0,1,2,3,4,5,...|\n",
      "|-17.065399625876015|(10,[0,1,2,3,4,5,...|\n",
      "|-17.026492264209548|(10,[0,1,2,3,4,5,...|\n",
      "| -16.71909683360509|(10,[0,1,2,3,4,5,...|\n",
      "| -16.26143027545273|(10,[0,1,2,3,4,5,...|\n",
      "|-16.151349351277112|(10,[0,1,2,3,4,5,...|\n",
      "| -16.08565904102149|(10,[0,1,2,3,4,5,...|\n",
      "+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|              label|            features|\n",
      "+-------------------+--------------------+\n",
      "|-28.571478869743427|(10,[0,1,2,3,4,5,...|\n",
      "|-26.805483428483072|(10,[0,1,2,3,4,5,...|\n",
      "|-26.736207182601724|(10,[0,1,2,3,4,5,...|\n",
      "| -23.51088409032297|(10,[0,1,2,3,4,5,...|\n",
      "|-20.212077258958672|(10,[0,1,2,3,4,5,...|\n",
      "| -19.16829262296376|(10,[0,1,2,3,4,5,...|\n",
      "|-18.845922472898582|(10,[0,1,2,3,4,5,...|\n",
      "| -18.27521356600463|(10,[0,1,2,3,4,5,...|\n",
      "|-17.494200356883344|(10,[0,1,2,3,4,5,...|\n",
      "|-16.692207021311106|(10,[0,1,2,3,4,5,...|\n",
      "| -15.72351561304857|(10,[0,1,2,3,4,5,...|\n",
      "|-15.348871155379253|(10,[0,1,2,3,4,5,...|\n",
      "|-14.762758252931127|(10,[0,1,2,3,4,5,...|\n",
      "|-13.976130931152703|(10,[0,1,2,3,4,5,...|\n",
      "|-13.867087895158768|(10,[0,1,2,3,4,5,...|\n",
      "|-13.772441561702871|(10,[0,1,2,3,4,5,...|\n",
      "| -13.15333560636553|(10,[0,1,2,3,4,5,...|\n",
      "|-12.558575788856189|(10,[0,1,2,3,4,5,...|\n",
      "|-12.491442077546413|(10,[0,1,2,3,4,5,...|\n",
      "|-12.479280211451497|(10,[0,1,2,3,4,5,...|\n",
      "+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_data = test_data.select('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unlabeled_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol='features', labelCol='label', predictionCol='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = correct_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "| -30.11076412180975|\n",
      "| -27.58530020532035|\n",
      "|-24.166993780540622|\n",
      "|-22.128914780662917|\n",
      "|-21.913061409208495|\n",
      "|  -16.3980036518122|\n",
      "|-22.817933367691552|\n",
      "|-19.711580801312174|\n",
      "|-14.266019229195605|\n",
      "|-16.981335086905872|\n",
      "| -19.89286248019693|\n",
      "| -13.21089442256958|\n",
      "|-14.132922334274744|\n",
      "|-13.659855746741185|\n",
      "|-13.711870936989563|\n",
      "| -19.29359349541181|\n",
      "|-12.824457440254776|\n",
      "| -13.48549760638075|\n",
      "|-14.439511178682995|\n",
      "|-11.971105355267202|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "RMSE: 9.958947447141474\n"
     ]
    }
   ],
   "source": [
    "test_results.residuals.show()\n",
    "print(\"RMSE: {}\".format(test_results.rootMeanSquaredError))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = correct_model.transform(unlabeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|          prediction|\n",
      "+--------------------+--------------------+\n",
      "|(10,[0,1,2,3,4,5,...|  1.5392852520663243|\n",
      "|(10,[0,1,2,3,4,5,...|  0.7798167768372809|\n",
      "|(10,[0,1,2,3,4,5,...| -2.5692134020611017|\n",
      "|(10,[0,1,2,3,4,5,...| -1.3819693096600552|\n",
      "|(10,[0,1,2,3,4,5,...|  1.7009841502498246|\n",
      "|(10,[0,1,2,3,4,5,...| -2.7702889711515626|\n",
      "|(10,[0,1,2,3,4,5,...|   3.972010894792971|\n",
      "|(10,[0,1,2,3,4,5,...|   1.436367235307543|\n",
      "|(10,[0,1,2,3,4,5,...|  -3.228181127687739|\n",
      "|(10,[0,1,2,3,4,5,...| 0.28912806559476684|\n",
      "|(10,[0,1,2,3,4,5,...|    4.16934686714836|\n",
      "|(10,[0,1,2,3,4,5,...|  -2.137976732809673|\n",
      "|(10,[0,1,2,3,4,5,...| -0.6298359186563837|\n",
      "|(10,[0,1,2,3,4,5,...| -0.3162751844115178|\n",
      "|(10,[0,1,2,3,4,5,...|-0.15521695816920533|\n",
      "|(10,[0,1,2,3,4,5,...|  5.5211519337089365|\n",
      "|(10,[0,1,2,3,4,5,...| -0.3288781661107554|\n",
      "|(10,[0,1,2,3,4,5,...|   0.926921817524561|\n",
      "|(10,[0,1,2,3,4,5,...|   1.948069101136583|\n",
      "|(10,[0,1,2,3,4,5,...| -0.5081748561842961|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTreeClassifier,GBTClassifier,RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify whether teh college is Public or Private"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('College.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- School: string (nullable = true)\n",
      " |-- Private: string (nullable = true)\n",
      " |-- Apps: integer (nullable = true)\n",
      " |-- Accept: integer (nullable = true)\n",
      " |-- Enroll: integer (nullable = true)\n",
      " |-- Top10perc: integer (nullable = true)\n",
      " |-- Top25perc: integer (nullable = true)\n",
      " |-- F_Undergrad: integer (nullable = true)\n",
      " |-- P_Undergrad: integer (nullable = true)\n",
      " |-- Outstate: integer (nullable = true)\n",
      " |-- Room_Board: integer (nullable = true)\n",
      " |-- Books: integer (nullable = true)\n",
      " |-- Personal: integer (nullable = true)\n",
      " |-- PhD: integer (nullable = true)\n",
      " |-- Terminal: integer (nullable = true)\n",
      " |-- S_F_Ratio: double (nullable = true)\n",
      " |-- perc_alumni: integer (nullable = true)\n",
      " |-- Expend: integer (nullable = true)\n",
      " |-- Grad_Rate: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(School='Abilene Christian University', Private='Yes', Apps=1660, Accept=1232, Enroll=721, Top10perc=23, Top25perc=52, F_Undergrad=2885, P_Undergrad=537, Outstate=7440, Room_Board=3300, Books=450, Personal=2200, PhD=70, Terminal=78, S_F_Ratio=18.1, perc_alumni=12, Expend=7041, Grad_Rate=60)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import VectorAssembler and Vectors\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['School',\n",
       " 'Private',\n",
       " 'Apps',\n",
       " 'Accept',\n",
       " 'Enroll',\n",
       " 'Top10perc',\n",
       " 'Top25perc',\n",
       " 'F_Undergrad',\n",
       " 'P_Undergrad',\n",
       " 'Outstate',\n",
       " 'Room_Board',\n",
       " 'Books',\n",
       " 'Personal',\n",
       " 'PhD',\n",
       " 'Terminal',\n",
       " 'S_F_Ratio',\n",
       " 'perc_alumni',\n",
       " 'Expend',\n",
       " 'Grad_Rate']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "  inputCols=['Apps',\n",
    "             'Accept',\n",
    "             'Enroll',\n",
    "             'Top10perc',\n",
    "             'Top25perc',\n",
    "             'F_Undergrad',\n",
    "             'P_Undergrad',\n",
    "             'Outstate',\n",
    "             'Room_Board',\n",
    "             'Books',\n",
    "             'Personal',\n",
    "             'PhD',\n",
    "             'Terminal',\n",
    "             'S_F_Ratio',\n",
    "             'perc_alumni',\n",
    "             'Expend',\n",
    "             'Grad_Rate'],\n",
    "              outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(School='Abilene Christian University', Private='Yes', Apps=1660, Accept=1232, Enroll=721, Top10perc=23, Top25perc=52, F_Undergrad=2885, P_Undergrad=537, Outstate=7440, Room_Board=3300, Books=450, Personal=2200, PhD=70, Terminal=78, S_F_Ratio=18.1, perc_alumni=12, Expend=7041, Grad_Rate=60, features=DenseVector([1660.0, 1232.0, 721.0, 23.0, 52.0, 2885.0, 537.0, 7440.0, 3300.0, 450.0, 2200.0, 70.0, 78.0, 18.1, 12.0, 7041.0, 60.0]))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"Private\", outputCol=\"PrivateIndex\")\n",
    "output_fixed = indexer.fit(output).transform(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = output_fixed.select(\"features\",'PrivateIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([1660.0, 1232.0, 721.0, 23.0, 52.0, 2885.0, 537.0, 7440.0, 3300.0, 450.0, 2200.0, 70.0, 78.0, 18.1, 12.0, 7041.0, 60.0]), PrivateIndex=0.0),\n",
       " Row(features=DenseVector([2186.0, 1924.0, 512.0, 16.0, 29.0, 2683.0, 1227.0, 12280.0, 6450.0, 750.0, 1500.0, 29.0, 30.0, 12.2, 16.0, 10527.0, 56.0]), PrivateIndex=0.0),\n",
       " Row(features=DenseVector([1428.0, 1097.0, 336.0, 22.0, 50.0, 1036.0, 99.0, 11250.0, 3750.0, 400.0, 1165.0, 53.0, 66.0, 12.9, 30.0, 8735.0, 54.0]), PrivateIndex=0.0),\n",
       " Row(features=DenseVector([417.0, 349.0, 137.0, 60.0, 89.0, 510.0, 63.0, 12960.0, 5450.0, 450.0, 875.0, 92.0, 97.0, 7.7, 37.0, 19016.0, 59.0]), PrivateIndex=0.0),\n",
       " Row(features=DenseVector([193.0, 146.0, 55.0, 16.0, 44.0, 249.0, 869.0, 7560.0, 4120.0, 800.0, 1500.0, 76.0, 72.0, 11.9, 2.0, 10922.0, 15.0]), PrivateIndex=0.0)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier,GBTClassifier,RandomForestClassifier\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(labelCol='PrivateIndex',featuresCol='features')\n",
    "rfc = RandomForestClassifier(labelCol='PrivateIndex',featuresCol='features')\n",
    "gbt = GBTClassifier(labelCol='PrivateIndex',featuresCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_model = dtc.fit(train_data)\n",
    "rfc_model = rfc.fit(train_data)\n",
    "gbt_model = gbt.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_predictions = dtc_model.transform(test_data)\n",
    "rfc_predictions = rfc_model.transform(test_data)\n",
    "gbt_predictions = gbt_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"PrivateIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_acc = acc_evaluator.evaluate(dtc_predictions)\n",
    "rfc_acc = acc_evaluator.evaluate(rfc_predictions)\n",
    "gbt_acc = acc_evaluator.evaluate(gbt_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the results!\n",
      "--------------------------------------------------------------------------------\n",
      "A single decision tree had an accuracy of: 88.48%\n",
      "--------------------------------------------------------------------------------\n",
      "A random forest ensemble had an accuracy of: 92.59%\n",
      "--------------------------------------------------------------------------------\n",
      "A ensemble using GBT had an accuracy of: 88.07%\n"
     ]
    }
   ],
   "source": [
    "print(\"Here are the results!\")\n",
    "print('-'*80)\n",
    "print('A single decision tree had an accuracy of: {0:2.2f}%'.format(dtc_acc*100))\n",
    "print('-'*80)\n",
    "print('A random forest ensemble had an accuracy of: {0:2.2f}%'.format(rfc_acc*100))\n",
    "print('-'*80)\n",
    "print('A ensemble using GBT had an accuracy of: {0:2.2f}%'.format(gbt_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Loads data.\n",
    "dataset = spark.read.csv(\"seeds_dataset.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(area=15.26, perimeter=14.84, compactness=0.871, length_of_kernel=5.763, width_of_kernel=3.312, asymmetry_coefficient=2.221, length_of_groove=5.22)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['area',\n",
       " 'perimeter',\n",
       " 'compactness',\n",
       " 'length_of_kernel',\n",
       " 'width_of_kernel',\n",
       " 'asymmetry_coefficient',\n",
       " 'length_of_groove']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_assembler = VectorAssembler(inputCols = dataset.columns, outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = vec_assembler.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize each feature to have unit standard deviation.\n",
    "final_data = scalerModel.transform(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains a k-means model.\n",
    "kmeans = KMeans(featuresCol='scaledFeatures',k=3)\n",
    "model = kmeans.fit(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[ 4.07497225 10.14410142 35.89816849 11.80812742  7.54416916  3.15410901\n",
      " 10.38031464]\n",
      "[ 6.35645488 12.40730852 37.41990178 13.93860446  9.7892399   2.41585013\n",
      " 12.29286107]\n",
      "[ 4.96198582 10.97871333 37.30930808 12.44647267  8.62880781  1.80061978\n",
      " 10.41913733]\n"
     ]
    }
   ],
   "source": [
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         1|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         0|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(final_data).select('prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Binary Classification on Spam/Good SMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"SMSSpamCollection\",inferSchema=True,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumnRenamed('_c0','class').withColumnRenamed('_c1','text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                text|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "|  ham|U dun say so earl...|\n",
      "|  ham|Nah I don't think...|\n",
      "| spam|FreeMsg Hey there...|\n",
      "|  ham|Even my brother i...|\n",
      "|  ham|As per your reque...|\n",
      "| spam|WINNER!! As a val...|\n",
      "| spam|Had your mobile 1...|\n",
      "|  ham|I'm gonna be home...|\n",
      "| spam|SIX chances to wi...|\n",
      "| spam|URGENT! You have ...|\n",
      "|  ham|I've been searchi...|\n",
      "|  ham|I HAVE A DATE ON ...|\n",
      "| spam|XXXMobileMovieClu...|\n",
      "|  ham|Oh k...i'm watchi...|\n",
      "|  ham|Eh u remember how...|\n",
      "|  ham|Fine if thats th...|\n",
      "| spam|England v Macedon...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn('length',length(data['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n",
      "|class|                text|length|\n",
      "+-----+--------------------+------+\n",
      "|  ham|Go until jurong p...|   111|\n",
      "|  ham|Ok lar... Joking ...|    29|\n",
      "| spam|Free entry in 2 a...|   155|\n",
      "|  ham|U dun say so earl...|    49|\n",
      "|  ham|Nah I don't think...|    61|\n",
      "| spam|FreeMsg Hey there...|   147|\n",
      "|  ham|Even my brother i...|    77|\n",
      "|  ham|As per your reque...|   160|\n",
      "| spam|WINNER!! As a val...|   157|\n",
      "| spam|Had your mobile 1...|   154|\n",
      "|  ham|I'm gonna be home...|   109|\n",
      "| spam|SIX chances to wi...|   136|\n",
      "| spam|URGENT! You have ...|   155|\n",
      "|  ham|I've been searchi...|   196|\n",
      "|  ham|I HAVE A DATE ON ...|    35|\n",
      "| spam|XXXMobileMovieClu...|   149|\n",
      "|  ham|Oh k...i'm watchi...|    26|\n",
      "|  ham|Eh u remember how...|    81|\n",
      "|  ham|Fine if thats th...|    56|\n",
      "| spam|England v Macedon...|   155|\n",
      "+-----+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|class|      avg(length)|\n",
      "+-----+-----------------+\n",
      "|  ham|71.45431945307645|\n",
      "| spam|138.6706827309237|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pretty Clear Difference\n",
    "data.groupby('class').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer,StopWordsRemover, CountVectorizer,IDF,StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n",
    "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
    "count_vec = CountVectorizer(inputCol='stop_tokens',outputCol='c_vec')\n",
    "idf = IDF(inputCol=\"c_vec\", outputCol=\"tf_idf\")\n",
    "ham_spam_to_num = StringIndexer(inputCol='class',outputCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_up = VectorAssembler(inputCols=['tf_idf','length'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use defaults\n",
    "nb = NaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_pipe = Pipeline(stages=[ham_spam_to_num,tokenizer,stopremove,count_vec,idf,clean_up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = data_prep_pipe.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = cleaner.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = clean_data.select(['label','features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(13424,[7,11,31,6...|\n",
      "|  0.0|(13424,[0,24,297,...|\n",
      "|  1.0|(13424,[2,13,19,3...|\n",
      "|  0.0|(13424,[0,70,80,1...|\n",
      "|  0.0|(13424,[36,134,31...|\n",
      "|  1.0|(13424,[10,60,139...|\n",
      "|  0.0|(13424,[10,53,103...|\n",
      "|  0.0|(13424,[125,184,4...|\n",
      "|  1.0|(13424,[1,47,118,...|\n",
      "|  1.0|(13424,[0,1,13,27...|\n",
      "|  0.0|(13424,[18,43,120...|\n",
      "|  1.0|(13424,[8,17,37,8...|\n",
      "|  1.0|(13424,[13,30,47,...|\n",
      "|  0.0|(13424,[39,96,217...|\n",
      "|  0.0|(13424,[552,1697,...|\n",
      "|  1.0|(13424,[30,109,11...|\n",
      "|  0.0|(13424,[82,214,47...|\n",
      "|  0.0|(13424,[0,2,49,13...|\n",
      "|  0.0|(13424,[0,74,105,...|\n",
      "|  1.0|(13424,[4,30,33,5...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training,testing) = clean_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_predictor = nb.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- class: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- length: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = spam_predictor.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(13424,[0,1,2,41,...|[-1095.7405555588...|[0.99999999999999...|       0.0|\n",
      "|  0.0|(13424,[0,1,4,50,...|[-834.44332386290...|[1.0,1.7510124086...|       0.0|\n",
      "|  0.0|(13424,[0,1,5,15,...|[-1003.0702062455...|[1.0,2.5804336480...|       0.0|\n",
      "|  0.0|(13424,[0,1,7,8,1...|[-879.21729294645...|[1.0,2.9425054127...|       0.0|\n",
      "|  0.0|(13424,[0,1,7,8,1...|[-1154.8501403550...|[1.0,1.0149703186...|       0.0|\n",
      "|  0.0|(13424,[0,1,9,14,...|[-538.88581517649...|[1.0,3.6655468330...|       0.0|\n",
      "|  0.0|(13424,[0,1,12,33...|[-466.88255263544...|[1.0,1.0628457672...|       0.0|\n",
      "|  0.0|(13424,[0,1,14,18...|[-1379.7263875076...|[1.0,8.3279409940...|       0.0|\n",
      "|  0.0|(13424,[0,1,14,78...|[-688.79826563448...|[1.0,1.6011120288...|       0.0|\n",
      "|  0.0|(13424,[0,1,15,20...|[-688.93799541091...|[1.0,1.1209926485...|       0.0|\n",
      "|  0.0|(13424,[0,1,17,19...|[-823.99497156689...|[1.0,3.4696696551...|       0.0|\n",
      "|  0.0|(13424,[0,1,18,20...|[-832.90702287011...|[1.0,1.6724811630...|       0.0|\n",
      "|  0.0|(13424,[0,1,23,63...|[-1308.8190088953...|[1.0,1.5183770540...|       0.0|\n",
      "|  0.0|(13424,[0,1,24,31...|[-357.67829963692...|[1.0,3.7705584076...|       0.0|\n",
      "|  0.0|(13424,[0,1,46,17...|[-1157.4247041543...|[1.08770273756349...|       1.0|\n",
      "|  0.0|(13424,[0,1,72,10...|[-668.97166936454...|[1.0,2.6056750129...|       0.0|\n",
      "|  0.0|(13424,[0,1,146,1...|[-254.12408809696...|[0.12953619004436...|       1.0|\n",
      "|  0.0|(13424,[0,1,498,5...|[-318.09108837108...|[0.99999999999987...|       0.0|\n",
      "|  0.0|(13424,[0,2,3,4,6...|[-1286.1724425806...|[1.0,1.8218056112...|       0.0|\n",
      "|  0.0|(13424,[0,2,3,6,9...|[-3294.0693131765...|[1.0,2.1608440089...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting spam was: 0.9191489871735747\n"
     ]
    }
   ],
   "source": [
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(\"Accuracy of model at predicting spam was: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
