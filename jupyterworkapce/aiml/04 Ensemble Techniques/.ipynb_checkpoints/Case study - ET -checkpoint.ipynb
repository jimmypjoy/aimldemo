{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study - Wine Quality Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context\n",
    "This datasets is related to red variants of the Portuguese \"Vinho Verde\" wine. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) \n",
    "variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\n",
    "\n",
    "These datasets can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are \n",
    "much more normal wines than excellent or poor ones)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset:\n",
    "https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009\n",
    "\n",
    "This dataset is also available from the UCI machine learning repository: https://archive.ics.uci.edu/ml/datasets/wine+quality "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement:\n",
    "    \n",
    "Wine Quality Prediction- Here, we will apply a method of assessing wine quality using a decision tree, and test it against the \n",
    "wine-quality dataset from the UC Irvine Machine Learning Repository.\n",
    "The wine dataset is a classic and very easy multi-class classification dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Instances: red wine - 1599\n",
    "\n",
    "## Attribute information:\n",
    "  - Input variables (based on physicochemical tests):\n",
    "      1. fixed acidity (tartaric acid - g / dm^3)\n",
    "      2. volatile acidity (acetic acid - g / dm^3)\n",
    "      3. citric acid (g / dm^3)\n",
    "      4. residual sugar (g / dm^3)\n",
    "      5. chlorides (sodium chloride - g / dm^3\n",
    "      6. free sulfur dioxide (mg / dm^3)\n",
    "      7. total sulfur dioxide (mg / dm^3)\n",
    "      8. density (g / cm^3)\n",
    "      9. pH\n",
    "      10. sulphates (potassium sulphate - g / dm3)\n",
    "      11. alcohol (% by volume)\n",
    "  - Output variable (based on sensory data): \n",
    "      - quality (score between 0 and 10)\n",
    "\n",
    "## Missing Attribute Values: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of attributes:\n",
    "\n",
    "   1 - fixed acidity: most acids involved with wine or fixed or nonvolatile (do not evaporate readily)\n",
    "\n",
    "   2 - volatile acidity: the amount of acetic acid in wine, which at too high of levels can lead to an unpleasant, vinegar taste\n",
    "\n",
    "   3 - citric acid: found in small quantities, citric acid can add 'freshness' and flavor to wines\n",
    "\n",
    "   4 - residual sugar: the amount of sugar remaining after fermentation stops, it's rare to find wines with less than 1 gram/liter and wines with greater than 45 grams/liter are considered sweet\n",
    "\n",
    "   5 - chlorides: the amount of salt in the wine\n",
    "\n",
    "   6 - free sulfur dioxide: the free form of SO2 exists in equilibrium between molecular SO2 (as a dissolved gas) and bisulfite ion; it prevents microbial growth and the oxidation of wine\n",
    "\n",
    "   7 - total sulfur dioxide: amount of free and bound forms of S02; in low concentrations, SO2 is mostly undetectable in wine, but at free SO2 concentrations over 50 ppm, SO2 becomes evident in the nose and taste of wine\n",
    "\n",
    "   8 - density: the density of water is close to that of water depending on the percent alcohol and sugar content\n",
    "\n",
    "   9 - pH: describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic); most wines are between 3-4 on the pH scale\n",
    "\n",
    "   10 - sulphates: a wine additive which can contribute to sulfur dioxide gas (S02) levels, wich acts as an antimicrobial and antioxidant\n",
    "\n",
    "   11 - alcohol: the percent alcohol content of the wine\n",
    "\n",
    "   Output variable (based on sensory data): \n",
    "   12 - quality (score between 0 and 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all necessary modules and load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas_profiling'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-407c054cae8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas_profiling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas_profiling'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import pandas_profiling\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "from yellowbrick.classifier import ClassificationReport, ROCAUC\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('winequality-red.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print first 10 observations from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always a good practice to eye-ball raw data to get a feel of the data in terms of number of structure of the file, number \n",
    "of attributes, types of attributes and a general idea of likely challenges in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the datatypes of each column and the shape of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the descriptive statistics of each & every column using describe() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using univariate analysis check the individual attributes for their basic statistic such as central values, spread, tails etc.\n",
    "What are your observations (any two attributes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df['quality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['residual_sugar'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use correlation method to observe the relationship between different variables and state your insights.\n",
    "Hint: Use seaborn plot and check the relationship between different variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "sns.heatmap(df.corr(),\n",
    "            annot=True,\n",
    "            linewidths=.5,\n",
    "            center=0,\n",
    "            cbar=False,\n",
    "            cmap=\"YlGnBu\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#levels of Y variable\n",
    "df['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine 7&8 together; combine 3 and 4 with 5 so that we have only 3 levels and a more balanced Y variable\n",
    "mapping = {7:8, 3:5, 4:5}\n",
    "\n",
    "df['quality'] = df['quality'].replace(mapping)\n",
    "df['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue = 'quality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the wine_df into training and test set in the ratio of 70:30 (Training:Test) based on dependent and independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into training and test set for independent attributes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = [col for col in df.columns if col != 'quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features], df['quality'], test_size=.3, random_state=22)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the decision tree model using “entropy” method of finding the split columns and fit it to training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoking the decision tree classifier function. Using 'entropy' method of finding the split columns. Other option \n",
    "# could be gini index.  \n",
    "\n",
    "model_entropy = DecisionTreeClassifier(criterion='entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_entropy.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the accuracy of the model & print the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train: %.2f\" % model_entropy.score(X_train, y_train))  # performance on train data\n",
    "print(\"Test: %.2f\" % model_entropy.score(X_test, y_test))  # performance on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is a high degree of overfitting in the model due to which the test accuracy drops drastically. This shows why decision trees are prone to overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularize/prune the decision tree by limiting the max. depth of trees and print the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pruned = DecisionTreeClassifier(criterion = \"entropy\", max_depth=4)\n",
    "clf_pruned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train: %.2f\" % clf_pruned.score(X_train, y_train))  # performance on train data\n",
    "print(\"Test: %.2f\" % clf_pruned.score(X_test, y_test))  # performance on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You might need to install pydotplus and graphviz packages for visualising the tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "export_graphviz(clf_pruned, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names = features,class_names=['5','6','8'])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "graph.write_png('wines_pruned.png')\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = clf_pruned.predict(X_train)\n",
    "preds_test = clf_pruned.predict(X_test)\n",
    "\n",
    "acc_DT = accuracy_score(y_test, preds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "pd.crosstab(y_test, preds_test, rownames=['Actual'], colnames=['Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize model performance with yellowbrick library\n",
    "viz = ClassificationReport(DecisionTreeClassifier(criterion = \"entropy\", max_depth=4))\n",
    "viz.fit(X_train, y_train)\n",
    "viz.score(X_test, y_test)\n",
    "viz.show()\n",
    "\n",
    "roc = ROCAUC(DecisionTreeClassifier(criterion = \"entropy\", max_depth=4))\n",
    "roc.fit(X_train, y_train)\n",
    "roc.score(X_test, y_test)\n",
    "roc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance metrics\n",
    "\n",
    "- __Precision__: Fraction of actuals per label that were correctly classified by the model\n",
    "- __Recall__: Fraction of predictions that were correctly classified by the model\n",
    "- __F1-score__: Weighted harmonic mean of the precision and recall. F1-score: 2 * (precision * recall) / (precision + recall)\n",
    "- __Support__: Number of occurrences of each class in y_test\n",
    "- __Accuracy__: Fraction of all observations that were correctly classified by the model\n",
    "- __Macro avg__: Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account\n",
    "- __Micro/weighted avg__: Calculate metrics globally by counting the total true positives, false negatives and false positives\n",
    "- __AUC Score__: Given a random observation from the dataset that belongs to a class, and a random observation that doesn't belong to a class, the AUC is the perecentage of time that our model will classify which is which correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ProbQuality_5'] = clf_pruned.predict_proba(df[features])[:,0]\n",
    "df['ProbQuality_6'] = clf_pruned.predict_proba(df[features])[:,1]\n",
    "df['ProbQuality_8'] = clf_pruned.predict_proba(df[features])[:,2]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When the tree is regularised, overfitting is reduced, but there is no increase in accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Calculating feature importance\n",
    "\n",
    "feat_importance = clf_pruned.tree_.compute_feature_importances(normalize=False)\n",
    "\n",
    "\n",
    "feat_imp_dict = dict(zip(features, clf_pruned.feature_importances_))\n",
    "feat_imp = pd.DataFrame.from_dict(feat_imp_dict, orient='index')\n",
    "feat_imp.sort_values(by=0, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the feature importance dataframe we can infer that alcohol, sulphate, volatile acidity and total sulfur dioxide are the variables that impact wine quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "resultsDf = pd.DataFrame({'Method':['Decision Tree'], 'accuracy': acc_DT})\n",
    "resultsDf = resultsDf[['Method', 'accuracy']]\n",
    "resultsDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the Random forest model and print the accuracy of Random forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfcl = RandomForestClassifier(n_estimators = 50)\n",
    "rfcl = rfcl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_RF = rfcl.predict(X_test)\n",
    "acc_RF = accuracy_score(y_test, pred_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempResultsDf = pd.DataFrame({'Method':['Random Forest'], 'accuracy': [acc_RF]})\n",
    "resultsDf = pd.concat([resultsDf, tempResultsDf])\n",
    "resultsDf = resultsDf[['Method', 'accuracy']]\n",
    "resultsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance with yellowbrick library\n",
    "viz = ClassificationReport(RandomForestClassifier(n_estimators = 50))\n",
    "viz.fit(X_train, y_train)\n",
    "viz.score(X_test, y_test)\n",
    "viz.show()\n",
    "\n",
    "roc = ROCAUC(RandomForestClassifier(n_estimators = 50))\n",
    "roc.fit(X_train, y_train)\n",
    "roc.score(X_test, y_test)\n",
    "roc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compared to the decision tree, we can see that the accuracy has significantly improved for the Random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Adaboost Ensemble Algorithm for the same data and print the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "abcl = AdaBoostClassifier(n_estimators = 100, learning_rate=0.1, random_state=22)\n",
    "abcl = abcl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_AB =abcl.predict(X_test)\n",
    "acc_AB = accuracy_score(y_test, pred_AB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempResultsDf = pd.DataFrame({'Method':['Adaboost'], 'accuracy': [acc_AB]})\n",
    "resultsDf = pd.concat([resultsDf, tempResultsDf])\n",
    "resultsDf = resultsDf[['Method', 'accuracy']]\n",
    "resultsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance with yellowbrick library\n",
    "viz = ClassificationReport(AdaBoostClassifier(n_estimators= 100, learning_rate=0.1, random_state=22))\n",
    "viz.fit(X_train, y_train)\n",
    "viz.score(X_test, y_test)\n",
    "viz.show()\n",
    "\n",
    "roc = ROCAUC(AdaBoostClassifier(n_estimators= 100, learning_rate=0.1, random_state=22))\n",
    "roc.fit(X_train, y_train)\n",
    "roc.score(X_test, y_test)\n",
    "roc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Bagging Classifier Algorithm and print the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bgcl = BaggingClassifier(n_estimators=50, max_samples= .7, bootstrap=True, oob_score=True, random_state=22)\n",
    "bgcl = bgcl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_BG = bgcl.predict(X_test)\n",
    "acc_BG = accuracy_score(y_test, pred_BG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempResultsDf = pd.DataFrame({'Method':['Bagging'], 'accuracy': [acc_BG]})\n",
    "resultsDf = pd.concat([resultsDf, tempResultsDf])\n",
    "resultsDf = resultsDf[['Method', 'accuracy']]\n",
    "resultsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance with yellowbrick library\n",
    "viz = ClassificationReport(BaggingClassifier(n_estimators=50, max_samples= .7, bootstrap=True, oob_score=True, random_state=22))\n",
    "viz.fit(X_train, y_train)\n",
    "viz.score(X_test, y_test)\n",
    "viz.show()\n",
    "\n",
    "roc = ROCAUC(BaggingClassifier(n_estimators=50, max_samples= .7, bootstrap=True, oob_score=True, random_state=22))\n",
    "roc.fit(X_train, y_train)\n",
    "roc.score(X_test, y_test)\n",
    "roc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply GradientBoost Classifier Algorithm for the same data and print the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbcl = GradientBoostingClassifier(n_estimators = 50, learning_rate = 0.1, random_state=22)\n",
    "gbcl = gbcl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_GB = gbcl.predict(X_test)\n",
    "acc_GB = accuracy_score(y_test, pred_GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tempResultsDf = pd.DataFrame({'Method':['Gradient Boost'], 'accuracy': [acc_GB]})\n",
    "resultsDf = pd.concat([resultsDf, tempResultsDf])\n",
    "resultsDf = resultsDf[['Method', 'accuracy']]\n",
    "resultsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance with yellowbrick library\n",
    "viz = ClassificationReport(GradientBoostingClassifier(n_estimators = 50, learning_rate = 0.1, random_state=22))\n",
    "viz.fit(X_train, y_train)\n",
    "viz.score(X_test, y_test)\n",
    "viz.show()\n",
    "\n",
    "roc = ROCAUC(GradientBoostingClassifier(n_estimators = 50, learning_rate = 0.1, random_state=22))\n",
    "roc.fit(X_train, y_train)\n",
    "roc.score(X_test, y_test)\n",
    "roc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For this dataset, Random forest and bagging models give the best results on test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function for visualizing classifier results\n",
    "def visClassifierResults(model_w_parameters):\n",
    "    viz = ClassificationReport(model_w_parameters)\n",
    "    viz.fit(X_train, y_train)\n",
    "    viz.score(X_test, y_test)\n",
    "    viz.show()\n",
    "\n",
    "    roc = ROCAUC(model_w_parameters)\n",
    "    roc.fit(X_train, y_train)\n",
    "    roc.score(X_test, y_test)\n",
    "    roc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visClassifierResults(GradientBoostingClassifier(n_estimators = 50, learning_rate = 0.1, random_state=22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visClassifierResults(RandomForestClassifier(n_estimators = 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
