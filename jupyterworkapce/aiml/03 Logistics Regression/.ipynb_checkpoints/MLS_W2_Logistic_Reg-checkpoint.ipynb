{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Case study on Credit Risk`\n",
    "\n",
    "\n",
    "## `Context: `\n",
    "Credit risk is nothing but the default in payment of any loan by the borrower. In Banking sector this is an important factor to \n",
    "be considered before approving the loan of an applicant.Dream Housing Finance company deals in all home loans. They have presence across all urban, semi urban and rural areas. Customer first apply for home loan after that company validates the customer eligibility for loan.\n",
    "\n",
    "\n",
    "## `Objective:`\n",
    "Company wants to automate the loan eligibility process (real time) based on customer detail provided while filling online application form. These details are Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others. To automate this process, they have given a problem to identify the customers segments, those are eligible for loan amount so that they can specifically target these customers. Here they have provided a partial data set.\n",
    "\n",
    "## `Attributes Information:`\n",
    "\n",
    "* Variable:\t      Description\n",
    "* Loan_ID:\t          Unique Loan ID\n",
    "* Gender:\t          Male/ Female\n",
    "* Married:\t          Applicant married (Y/N)\n",
    "* Dependents:\t      Number of dependents\n",
    "* Education:\t      Applicant Education (Graduate/ Under Graduate)\n",
    "* Self_Employed:\t  Self employed (Y/N)\n",
    "* ApplicantIncome:\t  Applicant income\n",
    "* CoapplicantIncome: Coapplicant income\n",
    "* LoanAmount:\t      Loan amount in thousands\n",
    "* Loan_Amount_Term:  Term of loan in months\n",
    "* Credit_History:\t  credit history meets guidelines\n",
    "* Property_Area:\t  Urban/ Semi Urban/ Rural\n",
    "* Loan_Status:\t      Loan approved (Y/N)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "\n",
    "## `Index`\n",
    "\n",
    "----------------------\n",
    "- <a href = #link1>Import Libraries and Load Dataset </a>\n",
    "\n",
    "\n",
    "- <a href = #link2>Univariate Analysis</a> \n",
    "\n",
    "\n",
    "- <a href = #link3>Null Values Treatment</a>\n",
    "\n",
    "\n",
    "- <a href = #link4>Bivariate Analysis</a> \n",
    "\n",
    "\n",
    "- <a href = #link6>Model building using Logistic Regression from Sklearn</a>\n",
    "\n",
    "\n",
    "- <a href = #link7>Improving Model Performance via Checking Parameters of Logistic Regression</a>\n",
    "\n",
    "\n",
    "- <a href = #link8>Business Insights</a>\n",
    "\n",
    "\n",
    "- <a href = #link5>Bonus Content</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Let's start coding!`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id = \"link1\"></a> Import Libraries and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# importing ploting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "# To enable plotting graphs in Jupyter notebook\n",
    "%matplotlib inline\n",
    "\n",
    "#importing seaborn for statistical plots\n",
    "import seaborn as sns\n",
    "\n",
    "#Let us break the X and y dataframes into training set and test set. For this we will use\n",
    "#Sklearn package's data splitting function which is based on random function\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# calculate accuracy measures and confusion matrix\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('CreditRisk.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Think about it:**\n",
    "\n",
    "- What do you interpret by looking at the data in the first 5 rows?\n",
    "- Do you see that each column is so different from the other. \n",
    "    - Numbers of different magnitude plus many columns with categorical values\n",
    "- There are a lot of columns with categorical data, how to convert this text to numerical value?\n",
    "    - Gender, Married, Education, Self_Employed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id = \"link2\"></a> Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Think about it:**\n",
    "\n",
    "- What do you interpret from the different Dtype (data-type) shown here:\n",
    "     - float64\n",
    "     - int64\n",
    "     - object\n",
    "\n",
    "     How will we deal with the object data-type?\n",
    "\n",
    "- We know that our dataset has 614 rows of data.\n",
    "- By looking at the above output of data.info(), we see that some columns are having less number of non-null values.\n",
    "    - What does this mean?\n",
    "        - It means some values are NULL (missing)\n",
    "\n",
    "    How to deal with NULL (missing) values here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets analysze the distribution of the various attribute\n",
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights:** `CoapplicantIncome and LoanAmount has 0's that can happen so no need to fill these values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique() # Number of unique values in a column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights:**\n",
    "- `Loan_Amount_Term is categorical as it has only 10 unique values`\n",
    "- `Loan_ID column has all unique values and it not a continous column so it has 614 categories one for each row and that would not provide any info to the algorithm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_df = df.drop('Loan_ID', axis =1 ) # dropping this column as it will be 1-1 mapping anyways\n",
    "cr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distplot for continous columns\n",
    "for i in ['ApplicantIncome','CoapplicantIncome','LoanAmount']:\n",
    "    sns.distplot(df[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights:**\n",
    "- `As you can see all the three are skewed but we won't treat it as bank are supposed to have skewed applicants, If we treat the data here it will increase bias`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_df['Loan_Amount_Term'].value_counts(normalize=True)\n",
    "\n",
    "# value counts gives us how many times does the value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights:**\n",
    "- `Loan_Amount_Term most of the values are 360, and rest categories have a very small percentage. So it won't add much value to create dummies of these columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Loan_Amount_Term is highly skewed - so we will delete this column\n",
    "cr_df.drop(['Loan_Amount_Term'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert X & Y variable to a categorical variable wherever relevant\n",
    "cr_df['Loan_Status'] = cr_df['Loan_Status'].astype('category')\n",
    "cr_df['Credit_History'] = cr_df['Credit_History'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Think about it:**\n",
    "\n",
    "- We already know that the Dtype (data-type) of the columns, which of them are object or category type. Try to visualize it using countplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in ['Gender','Married','Education','Self_Employed','Credit_History','Property_Area','Loan_Status']:\n",
    "#     sns.countplot(cr_df[i])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(cr_df.columns[cr_df.dtypes=='object']):   # checking value counts of all object type columns\n",
    "    print(cr_df[i].value_counts(normalize=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate baseline proportion - ratio of Yes to No to identify data imbalance\n",
    "prop_Y = cr_df['Loan_Status'].value_counts(normalize=True)\n",
    "print(prop_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights:**\n",
    "- `There is a slight imbalance in the data but no need to treat it`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id = \"link3\"></a> Null Values Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Think about it:**\n",
    "\n",
    "- How will try to fill these null values.\n",
    "- Is it posiible to draw some relationship between features to fill null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling Null Values\n",
    "\n",
    "cr_df['Credit_History'].fillna(0,inplace=True)\n",
    "\n",
    "cr_df['Self_Employed'].fillna('No',inplace=True) #replacing with mode\n",
    "\n",
    "cr_df['Dependents'].fillna('0',inplace=True) #replacing with mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights:**\n",
    "- `Credit_History missing values are filled with 0 assuming that they don't meet credit history guidlines .`\n",
    "\n",
    "- `Self_Employed missing values are replaced with mode, as maximum people are not self employed`\n",
    "\n",
    "- `Dependents are also filled with mode, assuming most of the people do not have dependents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing relationship between CoapplicantIncome and Gender\n",
    "\n",
    "cr_df.groupby('Gender')['CoapplicantIncome'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights:**\n",
    "- `When CoapplicantIncome is 0 and Gender is Female we should fill it with 0.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filling null values in gender whose CoapplicantIncome is 0 with female\n",
    "\n",
    "z = cr_df[cr_df['CoapplicantIncome']==0]\n",
    "\n",
    "for i in list(z[z['Gender'].isnull()].index):\n",
    "# this would give row numbers of places where gender is null and have CoapplicantIncome as 0\n",
    "    cr_df.loc[i,'Gender'] = 'Female'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_df['Gender'].fillna('Male',inplace=True) #replacing remaining values with mode as no other relationship found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drawing relationship between Loan_Status, Gender, Property_Area, Married\n",
    "\n",
    "df.groupby(['Loan_Status','Gender','Property_Area'])['Married'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_df[cr_df['Married'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights:**\n",
    "- `When Gender is Male Loan Status is 1 and Propert_Area is Urban then mostly they are married.(mode)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replacing with mode observed in above mentioned relationship:\n",
    "\n",
    "cr_df.loc[104,'Married'] = 'Yes'\n",
    "cr_df.loc[228,'Married'] = 'Yes'\n",
    "cr_df.loc[435,'Married'] = 'No'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id = \"link4\"></a> Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Think about it:**\n",
    "\n",
    "- In this case-study, we are going to apply Logistic Regression.\n",
    "- But even before we apply, let's have a look at the data to see how each attribute is different when loan status is yes or not.\n",
    "- To check it, we ll have to use groupby and crosstabs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us look at the target column which is 'Loan_Status' to understand how the data is distributed amongst the various values\n",
    "cr_df.groupby([\"Loan_Status\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_df.groupby([\"Loan_Status\"]).median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights:**\n",
    "- `No major difference in case of ApplicantIncome and LoanAmount values w.r.t. target variable`\n",
    "\n",
    "- `But CoapplicantIncome is higher for people who take loan`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Think about it**\n",
    "\n",
    "- Please try to look at the outputs of the codes given below and see if there is any relationship or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.crosstab(cr_df['Gender'],cr_df['Loan_Status'],normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.crosstab(cr_df['Married'],cr_df['Loan_Status'],normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.crosstab(cr_df['Education'],cr_df['Loan_Status'],normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(cr_df['Self_Employed'],cr_df['Loan_Status'],normalize='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights:**\n",
    "- `No difference due to self employed status Threfore dropping this column`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_df.drop('Self_Employed',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(cr_df['Property_Area'],cr_df['Loan_Status'],normalize='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights:**\n",
    "- `Semi Urban people are taking more loans`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(cr_df['Dependents'],cr_df['Loan_Status'],normalize='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights:**\n",
    "- `Number of dependents have no relationship with Loan_status therfore Dependents is a nominal categorical variable`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id = \"link6\"></a> Model building using Logistic Regression from Sklearn \n",
    "\n",
    "We will use the sklearn library to build the model and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define X and Y variables\n",
    "X = cr_df.drop('Loan_Status', axis=1)\n",
    "Y = cr_df[['Loan_Status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert categorical vriables to dummy variables\n",
    "X = pd.get_dummies(X, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Think about it**\n",
    "\n",
    "- How does drop first help and why we do it\n",
    "  - It reduces the number of columns and hence increases the computational time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Split into training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30,random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Think about it**\n",
    "\n",
    "- Why do we use random State\n",
    "  - Sklearn randomly shuffles the data so we get a slightly different answer. If we fix the random state and re-run the code then all the scores will stay the same for that random state.\n",
    "  - If we do not use random state our metrics will slightly differ in every run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, roc_auc_score,accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(random_state=7)\n",
    "logreg.fit(X_train, y_train)                    # fit the model on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = logreg.predict(X_test)              # Predicting the target variable on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe the predicted and observed classes in a dataframe.\n",
    "\n",
    "z = X_test.copy()\n",
    "z['Observed Loan Status'] = y_test\n",
    "z['Predicted Loan Status'] = y_predict\n",
    "z.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to get confusion matrix in a proper format\n",
    "def draw_cm( actual, predicted ):\n",
    "    cm = confusion_matrix( actual, predicted)\n",
    "    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [0,1] , yticklabels = [0,1] )\n",
    "    plt.ylabel('Observed')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trainig accuracy\",logreg.score(X_train,y_train))  \n",
    "print()\n",
    "print(\"Testing accuracy\",logreg.score(X_test, y_test))\n",
    "print()\n",
    "print('Confusion Matrix')\n",
    "print(draw_cm(y_test,y_predict))\n",
    "print()\n",
    "print(\"Recall:\",recall_score(y_test,y_predict))\n",
    "print()\n",
    "print(\"Precision:\",precision_score(y_test,y_predict))\n",
    "print()\n",
    "print(\"F1 Score:\",f1_score(y_test,y_predict))\n",
    "print()\n",
    "print(\"Roc Auc Score:\",roc_auc_score(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install yellowbrick\n",
    "\n",
    "# Additional\n",
    "\n",
    "#AUC ROC curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id = \"link7\"></a> Improving Model Performance via Checking Parameters of Logistic Regression\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "`This is an introduction to let you know that all the algorithms have parameters aand changing those parameters affects the model performance. How to regularize parameter is covered in FMST module.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Parameters of logistic regression\n",
    "logreg.get_params()\n",
    "\n",
    "#If we dont specify the parameters in the model it takes default value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Think about it**\n",
    "\n",
    "- How to improve model performance\n",
    "    - By checking the parameters and try changing it and then verify using model metrics whether performance increased or not.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running a loop to check different values of 'solver'\n",
    "# all solver can be used with l2, only 'liblinear' and 'saga' works with both 'l1' and 'l2'\n",
    "\n",
    "train_score=[]\n",
    "test_score=[]\n",
    "solver = ['newton-cg','lbfgs','liblinear','sag','saga']\n",
    "for i in solver:\n",
    "    model = LogisticRegression(random_state=42,penalty='l2', C = 0.75,solver=i)  # changing values of solver\n",
    "    model.fit(X_train, y_train) \n",
    "    y_predict = model.predict(X_test)     \n",
    "    train_score.append(round(model.score(X_train, y_train),3))\n",
    "    test_score.append(round(model.score(X_test, y_test),3))\n",
    "    \n",
    "print(solver)\n",
    "print()\n",
    "print(train_score)\n",
    "print()\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score=[]\n",
    "test_score=[]\n",
    "solver = ['liblinear','saga']   # changing values of solver which works with 'l1'\n",
    "for i in solver:\n",
    "    model = LogisticRegression(random_state=42,penalty='l1', C = 0.75,solver=i)  #changed penalty to 'l1'\n",
    "    model.fit(X_train, y_train) \n",
    "    y_predict = model.predict(X_test)     \n",
    "    train_score.append(round(model.score(X_train, y_train),3))\n",
    "    test_score.append(round(model.score(X_test, y_test),3))\n",
    "    \n",
    "print(solver)\n",
    "print()\n",
    "print(train_score)\n",
    "print()\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights:**\n",
    "- `Highest accuracy is same 'l1' with 'liblinear' and 'l2' with 'newton-cg'`\n",
    "\n",
    "choose any one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=42,penalty='l1',solver='liblinear',class_weight='balanced') # changing class weight to balanced\n",
    "\n",
    "model.fit(X_train, y_train) \n",
    "\n",
    "y_predict = model.predict(X_test)     \n",
    "\n",
    "print(\"Trainig accuracy\",model.score(X_train,y_train))  \n",
    "print()\n",
    "print(\"Testing accuracy\",model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights:**\n",
    "- `Testing accuracy increased and model is not overfit anymore so adding class weight from the model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running a loop to check different values of 'C'\n",
    "\n",
    "train_score=[]                                 \n",
    "test_score=[]\n",
    "C = [0.01,0.1,0.25,0.5,0.75,1]\n",
    "for i in C:\n",
    "    model = LogisticRegression(random_state=42,penalty='l1', solver='liblinear',class_weight='balanced', C=i)  # changing values of C\n",
    "    model.fit(X_train, y_train) \n",
    "    y_predict = model.predict(X_test)     \n",
    "    train_score.append(round(model.score(X_train,y_train),3)) # appending training accuracy in a blank list for every run of the loop\n",
    "    test_score.append(round(model.score(X_test, y_test),3))   # appending testing accuracy in a blank list for every run of the loop\n",
    "    \n",
    "print(C)\n",
    "print()\n",
    "print(train_score)\n",
    "print()\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights:**\n",
    "- `Best testing accuracy is obtained for C=0.5, which is default`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Therefore final model is\n",
    "model = LogisticRegression(random_state=42,penalty='l1', solver='liblinear', class_weight='balanced',C=0.5) \n",
    "model.fit(X_train, y_train)\n",
    "y_predict = model.predict(X_test)\n",
    "print(\"Trainig accuracy\",model.score(X_train,y_train))  \n",
    "print()\n",
    "print(\"Testing accuracy\",model.score(X_test, y_test))\n",
    "print()\n",
    "print('Confusion Matrix')\n",
    "print(draw_cm(y_test,y_predict))\n",
    "print()\n",
    "print(\"Recall:\",recall_score(y_test,y_predict))\n",
    "print()\n",
    "print(\"Precision:\",precision_score(y_test,y_predict))\n",
    "print()\n",
    "print(\"F1 Score:\",f1_score(y_test,y_predict))\n",
    "print()\n",
    "print(\"Roc Auc Score:\",roc_auc_score(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional\n",
    "\n",
    "from yellowbrick.classifier import ClassificationReport, ROCAUC\n",
    "# Visualize model performance with yellowbrick library\n",
    "viz = ClassificationReport(model)\n",
    "viz.fit(X_train, y_train)\n",
    "viz.score(X_test, y_test)\n",
    "viz.show()\n",
    "\n",
    "roc = ROCAUC(model)\n",
    "roc.fit(X_train, y_train)\n",
    "roc.score(X_test, y_test)\n",
    "roc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id = \"link8\"></a>Business Insights "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Confusion matrix means`\n",
    "\n",
    "*True Positive (observed=1,predicted=1):*\n",
    "\n",
    "Predicted that home loan will be granted and the customer was eligibile for loan\n",
    "\n",
    "*False Positive (observed=0,predicted=1):*\n",
    "\n",
    "Predicted that home loan will be granted and the customer was not eligibile for loan\n",
    "\n",
    "*True Negative (observed=0,predicted=0):*\n",
    "\n",
    "Predicted that home loan will not be granted and the customer was not eligibile for loan\n",
    "\n",
    "*False Negative (observed=1,predicted=0):*\n",
    "\n",
    "Predicted that home loan will not be granted and the customer was eligibile for loan\n",
    "\n",
    "Here the bank wants to give loan to the people who are eligible for the home loan i.e. **less number of False Positive**, if FP is high bank would lose money. So that the bank doesn't lose money on the people who are not eligible for the loan. Hence **Precision is the important metric**.\n",
    "\n",
    "In case of False negative bank will lose few customers but that okay because the bank would want to retain money more than customers who are not eligible for loan.\n",
    "\n",
    "After achieving the desired accuracy we can deploy the model for practical use. As in the bank now can predict who is eligible for home loan. They can use the model for upcoming customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id = \"link5\"></a>Bonus Content\n",
    "\n",
    "Model building using Statsmodel.api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the logistic regression model\n",
    "import statsmodels.api as sm\n",
    "\n",
    "logit = sm.Logit(y_train, sm.add_constant(X_train))\n",
    "lg = logit.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary of logistic regression\n",
    "from scipy import stats\n",
    "stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\n",
    "print(lg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of Pseudo R^2\n",
    "\n",
    "A pseudo R^2 of 17% indicates that 17% of the uncertainty of the intercept only model is explained by the full model\n",
    "\n",
    "#### Calculate the odds ratio from the coef using the formula odds ratio=exp(coef)\n",
    "\n",
    "#### Calculate the probability from the odds ratio using the formula probability = odds / (1+odds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Odds Ratio, probability\n",
    "##create a data frame to collate Odds ratio, probability and p-value of the coef\n",
    "lgcoef = pd.DataFrame(lg.params, columns=['coef'])\n",
    "lgcoef.loc[:, \"Odds_ratio\"] = np.exp(lgcoef.coef)\n",
    "lgcoef['probability'] = lgcoef['Odds_ratio']/(1+lgcoef['Odds_ratio'])\n",
    "lgcoef['pval']=lg.pvalues\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIlter by significant p-value (pval <0.1) and sort descending by Odds ratio\n",
    "lgcoef = lgcoef.sort_values(by=\"Odds_ratio\", ascending=False)\n",
    "pval_filter = lgcoef['pval']<=0.1\n",
    "lgcoef[pval_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights:**\n",
    "- `Customers with credit history 1 have a 89% probability of defaulting the loan`\n",
    "\n",
    "- `Customers in semiurban areas have an odds of 2.50 times to default`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix:\n",
    "\n",
    "- **warnings.filterwarnings(\"ignore\")** : Never print matching warnings.\n",
    "\n",
    "- **Pandas** : Pandas is an open source library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "\n",
    "- **Numpy** : The fundamental package for scientific computing with Python.\n",
    "\n",
    "- **Matplotlib** : Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python.\n",
    "\n",
    "- **Seaborn** : Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\n",
    "\n",
    "- **pairplot** : Plot pairwise relationships in a dataset.\n",
    "\n",
    "- **boxplot** : Draw a box plot to show distributions with respect to categories.\n",
    "\n",
    "- **distplot** : Flexibly plot a univariate distribution of observations.\n",
    "\n",
    "- **pandas.DataFrame.corr** : Compute pairwise correlation of columns, excluding NA/null values.\n",
    "\n",
    "- **sklearn.linear_model.LogisticRegression** : Logistic Regression (aka logit, MaxEnt) classifier.\n",
    "\n",
    "- **scipy.stats** : This module contains a large number of probability distributions as well as a growing library of statistical functions.\n",
    "\n",
    "- **statsmodels** : It is a Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration.\n",
    "    - An extensive list of result statistics are available for each estimator.\n",
    "    - The results are tested against existing statistical packages to ensure that they are correct.\n",
    "    - The online documentation is hosted at [statsmodels.org](https://www.statsmodels.org/stable/index.html).\n",
    "\n",
    "- **yellowbrick** : Yellowbrick extends the Scikit-Learn API to make model selection and hyperparameter tuning easier. Under the hood, it  uses Matplotlib"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
