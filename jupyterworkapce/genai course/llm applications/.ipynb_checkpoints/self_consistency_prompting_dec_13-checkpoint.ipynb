{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4118719c-46a4-4cca-be96-ff30606d94e0",
   "metadata": {
    "id": "4118719c-46a4-4cca-be96-ff30606d94e0"
   },
   "source": [
    "### Self Consistency Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaa35cd-d404-4d57-9f7b-126a5899aa32",
   "metadata": {
    "id": "eaaa35cd-d404-4d57-9f7b-126a5899aa32"
   },
   "source": [
    "[Self-consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/pdf/2203.11171.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770fefbd-1df5-4602-8e66-ec57fbe491f7",
   "metadata": {
    "id": "770fefbd-1df5-4602-8e66-ec57fbe491f7"
   },
   "source": [
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "l9Qioy186V7f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l9Qioy186V7f",
    "outputId": "118c15fb-9259-40dd-924f-d62c2b6e1ad7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_openai\n",
      "  Downloading langchain_openai-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.3.22)\n",
      "Collecting openai<2.0.0,>=1.55.3 (from langchain_openai)\n",
      "  Downloading openai-1.57.3-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain_openai) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain_openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain_openai) (0.1.147)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain_openai) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain_openai) (2.10.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain_openai) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain_openai) (4.12.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain_openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain_openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain_openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain_openai) (4.66.6)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.55.3->langchain_openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.55.3->langchain_openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain_openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain_openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain_openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain_openai) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.21->langchain_openai) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.21->langchain_openai) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.21->langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.21->langchain_openai) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.2.3)\n",
      "Downloading langchain_openai-0.2.12-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.57.3-py3-none-any.whl (390 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.2/390.2 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken, openai, langchain_openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.54.5\n",
      "    Uninstalling openai-1.54.5:\n",
      "      Successfully uninstalled openai-1.54.5\n",
      "Successfully installed langchain_openai-0.2.12 openai-1.57.3 tiktoken-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4836ee1-70fc-43ea-a9b2-5278091ffc24",
   "metadata": {
    "id": "f4836ee1-70fc-43ea-a9b2-5278091ffc24",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the os module to interact with the operating system\n",
    "import os\n",
    "\n",
    "# Import the ChatOpenAI class from the langchain_openai package\n",
    "# This class is used to interact with OpenAI's chat models\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e23e4cc-112c-47e9-b327-a03a57672b54",
   "metadata": {
    "id": "8e23e4cc-112c-47e9-b327-a03a57672b54"
   },
   "source": [
    "### Instantiating the OpenAI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "GOnlmhee6GnE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GOnlmhee6GnE",
    "outputId": "eedc2fed-da8e-4548-b059-5767a0fef3ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# prompt: mount drive\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dbc6e4a-a14a-4c42-bf92-5e89078dcbf7",
   "metadata": {
    "id": "7dbc6e4a-a14a-4c42-bf92-5e89078dcbf7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the OPENAI_API_KEY environment variable by reading the API key from a file named 'key.txt' located in the parent directory\n",
    "os.environ[\"OPENAI_API_KEY\"] = open('/content/drive/MyDrive/GenAI classes/Gen AI tobeshared/Demo Notebooks/key.txt','r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a0d370b-0e13-4118-8bf1-5fd8363051d4",
   "metadata": {
    "id": "0a0d370b-0e13-4118-8bf1-5fd8363051d4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize a ChatOpenAI instance with a temperature of 0.0\n",
    "# Setting the temperature to 0.0 makes the model's output more deterministic,\n",
    "# meaning it will prioritize the most confident prediction over others.\n",
    "llm = ChatOpenAI(temperature = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c259cc-7068-4130-86ff-da7861467167",
   "metadata": {
    "id": "c1c259cc-7068-4130-86ff-da7861467167"
   },
   "source": [
    "### Defining the Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90351640-e489-40c0-96fc-83ec5bd3624c",
   "metadata": {
    "id": "90351640-e489-40c0-96fc-83ec5bd3624c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a template for prompts that includes both system and user messages\n",
    "# This template is used to structure the input to the chat model, ensuring that both system messages (e.g., instructions or context) and user messages are included in the conversation.\n",
    "prompt_template = \"\"\"\n",
    "{system_message}\n",
    "{user_message}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "215b3969-c8eb-48fd-ac9b-5b20edb81aa8",
   "metadata": {
    "id": "215b3969-c8eb-48fd-ac9b-5b20edb81aa8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a system message that instructs the assistant on its role and behavior\n",
    "# This message sets the context for the assistant to answer queries on financial information without repeating the question and to focus solely on the user's query.\n",
    "system_message = \"\"\"\n",
    "You are an assistant tasked to answer queries on financial information.\n",
    "Do not repeat the question. Only answer the question presented by the user.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "130ce9ea-9e73-405b-a10c-6616abd5fa9d",
   "metadata": {
    "id": "130ce9ea-9e73-405b-a10c-6616abd5fa9d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a template for generating answers based on a given context and question\n",
    "# This template instructs the model to use the provided context to generate a specified number of distinct answers to a given question.\n",
    "# The answers should be presented in numbered bullet points, focusing solely on the answers without additional context or explanation.\n",
    "\n",
    "answers_template = \"\"\"\n",
    "Context:\n",
    "{context}\n",
    "===\n",
    "Using the context above generate {num_answers} distinct answers to the following question:\n",
    "Question:\n",
    "{question}.\n",
    "\n",
    "Arrange your answers in numbered bullet points.\n",
    "Present only the answers in bullet points.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e62688e-95a1-4a28-9f1c-999bb63e647e",
   "metadata": {
    "id": "8e62688e-95a1-4a28-9f1c-999bb63e647e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a context string that provides an overview of Tesla's financial performance\n",
    "tesla_annual_report_context =\"\"\"\n",
    "In 2022, we recognized total revenues of $81.46 billion, respectively, representing an increase of $27.64 billion, compared to the prior year.\n",
    "We continue to ramp production, build new manufacturing capacity and expand our operations to enable increased deliveries and deployments of our\n",
    "products and further revenue growth.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97955af4-3ba2-4978-a6f6-5132beeaa1bd",
   "metadata": {
    "id": "97955af4-3ba2-4978-a6f6-5132beeaa1bd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "factual_question = \"What was the increase in annual revenue in 2022 compared to 2021?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6038c71b-ac0d-488e-beed-0a708571dfc1",
   "metadata": {
    "id": "6038c71b-ac0d-488e-beed-0a708571dfc1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Construct a prompt for the chat model by formatting the prompt template with the system message and a user message\n",
    "# The user message is generated by formatting the answers template with the Tesla annual report context, a factual question, and specifying the number of answers to generate\n",
    "# This results in a structured prompt that instructs the model to use the provided context to generate answers to a specific question, with the answers formatted as numbered\n",
    "# bullet points\n",
    "answers_prompt = prompt_template.format(\n",
    "    system_message=system_message,\n",
    "    user_message=answers_template.format(\n",
    "        context=tesla_annual_report_context,\n",
    "        question=factual_question,\n",
    "        num_answers=3\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aab8fa7d-743a-4b80-a3a8-46aefe94e45c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aab8fa7d-743a-4b80-a3a8-46aefe94e45c",
    "outputId": "208eb91a-ec28-4030-f66d-71743b2278af",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "You are an assistant tasked to answer queries on financial information.\n",
      "Do not repeat the question. Only answer the question presented by the user.\n",
      "\n",
      "\n",
      "Context:\n",
      "\n",
      "In 2022, we recognized total revenues of $81.46 billion, respectively, representing an increase of $27.64 billion, compared to the prior year.\n",
      "We continue to ramp production, build new manufacturing capacity and expand our operations to enable increased deliveries and deployments of our\n",
      "products and further revenue growth.\n",
      "\n",
      "===\n",
      "Using the context above generate 3 distinct answers to the following question:\n",
      "Question:\n",
      "What was the increase in annual revenue in 2022 compared to 2021?.\n",
      "\n",
      "Arrange your answers in numbered bullet points.\n",
      "Present only the answers in bullet points.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing the prompt\n",
    "print(answers_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd46518-8acd-4413-b9c9-2859b8606154",
   "metadata": {
    "id": "1dd46518-8acd-4413-b9c9-2859b8606154"
   },
   "source": [
    "### Generating the Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f537255-14e9-4727-9fef-33069ee0938a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2f537255-14e9-4727-9fef-33069ee0938a",
    "outputId": "11036c1e-dcfc-49a2-fc99-63dd46dbb0e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-4cc353cd8b11>:2: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(llm.predict(answers_prompt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- The increase in annual revenue in 2022 compared to 2021 was $27.64 billion.\n",
      "- Total revenues in 2022 were $81.46 billion, representing an increase of $27.64 billion from the prior year.\n",
      "- The increase in revenue from 2021 to 2022 was $27.64 billion.\n"
     ]
    }
   ],
   "source": [
    "# generating the answers\n",
    "print(llm.predict(answers_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbb9cd67-889e-4c61-ada3-dfb86fc87a52",
   "metadata": {
    "id": "cbb9cd67-889e-4c61-ada3-dfb86fc87a52",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# storing the above answers in a variable for further prompting\n",
    "factual_answers = llm.predict(answers_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58c3048-734c-41e8-b80b-667d5a9c2c88",
   "metadata": {
    "id": "b58c3048-734c-41e8-b80b-667d5a9c2c88"
   },
   "source": [
    "### Defining Self Consistency Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bf075e4-ee3f-4c02-9699-8dffe2db7e92",
   "metadata": {
    "id": "8bf075e4-ee3f-4c02-9699-8dffe2db7e92",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a template for evaluating the consistency of generated answers\n",
    "# This template is designed to present a set number of answers to a specific question,\n",
    "# instructing the evaluator to identify the most frequently occurring answer among the provided options.\n",
    "# The final answer should be presented in a clear and concise format, focusing solely on the most common solution.\n",
    "\n",
    "consistency_template = \"\"\"\n",
    "Here are {num_answers} answers to the question mentioned below:\n",
    "Question:\n",
    "{question}\n",
    "Answers:\n",
    "{answers}\n",
    "\n",
    "Observe the answers mentioned above and choose the answer that occurs most.\n",
    "Present only the most frequent solution in the following format.\n",
    "Final Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77da0635-afa2-4fa1-91a6-537c8ffe92a3",
   "metadata": {
    "id": "77da0635-afa2-4fa1-91a6-537c8ffe92a3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Construct a prompt for evaluating the consistency of generated answers by formatting the prompt template with the system message and a user message\n",
    "# The user message is generated by formatting the consistency template with the number of answers, the factual question, and the factual answers provided\n",
    "# This results in a structured prompt that instructs the model to present a set number of answers to a specific question and then choose the most frequent answer among them\n",
    "consistency_prompt = prompt_template.format(\n",
    "    system_message=system_message,\n",
    "    user_message=consistency_template.format(\n",
    "        num_answers=3,\n",
    "        question=factual_question,\n",
    "        answers=factual_answers\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be95b633-d541-4762-b5ac-390b7a769a61",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "be95b633-d541-4762-b5ac-390b7a769a61",
    "outputId": "93af2002-623d-4ecb-a1d6-685dc9e4e5c8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "You are an assistant tasked to answer queries on financial information.\n",
      "Do not repeat the question. Only answer the question presented by the user.\n",
      "\n",
      "\n",
      "Here are 3 answers to the question mentioned below:\n",
      "Question:\n",
      "What was the increase in annual revenue in 2022 compared to 2021?\n",
      "Answers:\n",
      "\n",
      "- The increase in annual revenue in 2022 compared to 2021 was $27.64 billion.\n",
      "- Total revenues in 2022 were $81.46 billion, representing an increase of $27.64 billion from the prior year.\n",
      "- The increase in revenue from 2021 to 2022 was $27.64 billion.\n",
      "\n",
      "Observe the answers mentioned above and choose the answer that occurs most.\n",
      "Present only the most frequent solution in the following format.\n",
      "Final Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing the consistency prompt\n",
    "print(consistency_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f40b8-9014-4b43-a2eb-2513dbdae906",
   "metadata": {
    "id": "328f40b8-9014-4b43-a2eb-2513dbdae906"
   },
   "source": [
    "### Generating Final Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "423e7822-a356-4560-8b2f-1c2e3ad3b394",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "423e7822-a356-4560-8b2f-1c2e3ad3b394",
    "outputId": "13aed137-785e-4510-819d-be6d220a399c",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The increase in annual revenue in 2022 compared to 2021 was $27.64 billion.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generating the final answer\n",
    "llm.predict(consistency_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037f9258-2206-412f-a907-66250723d8df",
   "metadata": {
    "id": "037f9258-2206-412f-a907-66250723d8df"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": ".m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m119"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
